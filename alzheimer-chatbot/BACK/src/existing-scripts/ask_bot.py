import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from deep_translator import GoogleTranslator
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ğŸ” Tradutores
pt_to_en = GoogleTranslator(source='pt', target='en')
en_to_pt = GoogleTranslator(source='en', target='pt')

# ğŸ“ Caminho do ChromaDB
chroma_client = chromadb.PersistentClient(path="chroma_db")

# ğŸ”  FunÃ§Ã£o de embedding
embedding_function = SentenceTransformerEmbeddingFunction(model_name="all-distilroberta-v1")

# ğŸ§  Conecta Ã  coleÃ§Ã£o
collection = chroma_client.get_or_create_collection(
    name="alzheimers_final",
    embedding_function=embedding_function
)

# ğŸ“¦ Verifica se hÃ¡ dados
if collection.count() == 0:
    print("âš ï¸ A coleÃ§Ã£o 'alzheimers_final' estÃ¡ vazia. Rode build_vector_db.py primeiro.")
    exit()

# âš™ï¸ Modelo
model_id = "tiiuae/falcon-rw-1b"
try:
    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir="./hf_cache")
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir="./hf_cache")
    model.eval()
except Exception as e:
    print(f"âŒ Erro ao carregar o modelo: {e}")
    exit()

# ğŸ¤– GeraÃ§Ã£o de resposta
def gerar_resposta(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=100,
            pad_token_id=tokenizer.eos_token_id,
            no_repeat_ngram_size=2
        )

    resposta = tokenizer.decode(output[0], skip_special_tokens=True)
    return resposta[len(prompt):].strip()

# ğŸŸ¢ Interface
print(f"ğŸ“¦ Total de documentos na coleÃ§Ã£o: {collection.count()}")
print("Bem-vindo ao ALOIS CHAT! ğŸ‘‹ (Digite 'sair' para encerrar o chat)")

while True:
    pergunta_pt = input("Cuidador: ").strip()
    if pergunta_pt.lower() == "sair":
        print("ğŸ‘‹ AtÃ© logo! O ALOIS CHAT foi encerrado.")
        break

    if not pergunta_pt:
        print("âš ï¸ Por favor, digite uma pergunta vÃ¡lida.\n")
        continue

    try:
        # ğŸ” TraduÃ§Ã£o da pergunta
        pergunta_en = pt_to_en.translate(pergunta_pt)

        # ğŸ” Busca vetorial
        resultados = collection.query(
            query_texts=[pergunta_en],
            n_results=3
        )

        documentos = resultados.get("documents", [[]])[0]
        distancias = resultados.get("distances", [[]])[0]

        print("[DEBUG] Documentos retornados:")
        for doc, dist in zip(documentos, distancias):
            print(f"- DistÃ¢ncia: {dist:.2f} | Trecho: {doc[:80]}...")

        # ğŸ” Seleciona o mais relevante
        contexto = documentos[0] if documentos else "No context found."

        prompt = (
            f"Context: {contexto}\n\n"
            f"Question: {pergunta_en}\n\n"
            f"Answer clearly, based only on the context above."
        )

        print("[DEBUG] Gerando resposta...")
        resposta_en = gerar_resposta(prompt)

        # ğŸ” TraduÃ§Ã£o de volta
        try:
            resposta_pt = en_to_pt.translate(resposta_en)
        except:
            resposta_pt = resposta_en

        print(f"\nğŸ“š Resposta baseada nos estudos e LLM:\n{resposta_pt}\n")

    except Exception as e:
        print(f"âŒ Erro: {e}\n")
